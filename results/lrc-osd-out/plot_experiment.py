#!/usr/bin/env python3
"""
Plot experiment throughput over time with failure/recovery markers
"""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime
import sys
import os

def parse_event_log(event_file):
    """Extract key event timestamps from event log"""
    events = {}
    
    with open(event_file, 'r') as f:
        for line in f:
            line = line.strip()
            if 'FAILURE EVENT' in line:
                timestamp = line.split('[')[1].split(']')[0]
                events['failure'] = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')
            elif 'RECOVERY STARTED' in line:
                timestamp = line.split(': ')[1].strip()
                events['recovery_start'] = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')
            elif 'RECOVERY COMPLETE' in line:
                timestamp = line.split(': ')[1].strip()
                events['recovery_complete'] = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')
            elif 'EXPERIMENT START' in line:
                timestamp = line.split('[')[1].split(']')[0]
                events['start'] = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')
    
    return events

def parse_throughput_data(csv_file, events, num_ranks=8):
    """Parse per-step throughput CSV generated by analysis"""
    
    if not os.path.exists(csv_file):
        print(f"Warning: Throughput CSV not found: {csv_file}")
        return pd.DataFrame(columns=['time', 'throughput', 'throughput_smooth', 'step'])
    
    # Read the CSV with per-step metrics
    df = pd.read_csv(csv_file)
    
    if df.empty:
        print("Warning: Throughput CSV is empty")
        return pd.DataFrame(columns=['time', 'throughput', 'throughput_smooth', 'step'])
    
    # Use the cumulative read time as our time axis
    df['time'] = df['cumulative_read_time']
    
    # Multiply per-rank throughput by number of ranks for aggregate throughput
    df['throughput'] = df['read_throughput_mb_s'] * num_ranks
    
    # Smooth with rolling average
    df['throughput_smooth'] = df['throughput'].rolling(window=5, center=True).mean()
    
    print(f"Using {num_ranks} ranks: aggregate throughput = per-rank Ã— {num_ranks}")
    
    return df

def plot_experiment(result_dir):
    """Generate throughput plot with event markers"""
    
    event_file = os.path.join(result_dir, 'experiment_events.log')
    csv_file = '/mnt/rbd-lrc/gray-scott/analysis/pdf-lrc-osdout.bp_throughput.csv'
    output_file = os.path.join(result_dir, 'throughput_plot.png')
    
    if not os.path.exists(event_file):
        print(f"Error: Event log not found: {event_file}")
        return
    
    # Parse data
    print("Parsing event log...")
    events = parse_event_log(event_file)
    
    print(f"Parsing throughput CSV: {csv_file}")
    df = parse_throughput_data(csv_file, events, num_ranks=8)
    
    if df.empty:
        print("Error: No throughput data available")
        return
    
    # Create plot
    fig, ax = plt.subplots(figsize=(14, 7))
    
    # Plot throughput
    ax.plot(df['time'], df['throughput_smooth'], 'b-', linewidth=2, label='Read Throughput (smoothed)', alpha=0.8)
    ax.plot(df['time'], df['throughput'], 'b-', linewidth=0.5, alpha=0.3, label='Read Throughput (raw)')
    
    # Mark events
    if 'failure' in events:
        failure_time = (events['failure'] - events['start']).total_seconds()
        ax.axvline(failure_time, color='red', linestyle='--', linewidth=2, 
                   label=f'OSD Failure ({failure_time:.1f}s)', alpha=0.8)
        ax.text(failure_time, ax.get_ylim()[1]*0.95, 'Failure', 
                rotation=90, va='top', ha='right', color='red', fontsize=10, fontweight='bold')
    
    if 'recovery_start' in events:
        recovery_start_time = (events['recovery_start'] - events['start']).total_seconds()
        ax.axvline(recovery_start_time, color='orange', linestyle='--', linewidth=2,
                   label=f'Recovery Start ({recovery_start_time:.1f}s)', alpha=0.8)
        ax.text(recovery_start_time, ax.get_ylim()[1]*0.90, 'Recovery\nStart', 
                rotation=90, va='top', ha='right', color='orange', fontsize=10, fontweight='bold')
    
    if 'recovery_complete' in events:
        recovery_end_time = (events['recovery_complete'] - events['start']).total_seconds()
        ax.axvline(recovery_end_time, color='green', linestyle='--', linewidth=2,
                   label=f'Recovery Complete ({recovery_end_time:.1f}s)', alpha=0.8)
        ax.text(recovery_end_time, ax.get_ylim()[1]*0.85, 'Recovery\nComplete', 
                rotation=90, va='top', ha='right', color='green', fontsize=10, fontweight='bold')
    
    # Formatting
    ax.set_xlabel('Time (seconds since start)', fontsize=12)
    ax.set_ylabel('Aggregate Read Throughput (MB/s)', fontsize=12)
    ax.set_title('LRC (k=6,m=2,l=4) ADIOS2 Gray-Scott: Read Throughput During OSD Failure and Recovery\n(8 MPI Ranks)', 
                 fontsize=14, fontweight='bold')
    ax.legend(loc='upper right', fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # Calculate statistics
    if 'failure' in events and 'recovery_complete' in events:
        pre_failure_df = df[df['time'] < failure_time]
        during_recovery_df = df[(df['time'] >= failure_time) & (df['time'] < recovery_end_time)]
        post_recovery_df = df[df['time'] >= recovery_end_time]
        
        if len(pre_failure_df) > 0:
            avg_baseline = pre_failure_df['throughput_smooth'].mean()
        else:
            avg_baseline = 0
            
        if len(during_recovery_df) > 0:
            avg_degraded = during_recovery_df['throughput_smooth'].mean()
        else:
            avg_degraded = 0
            
        if len(post_recovery_df) > 0:
            avg_recovered = post_recovery_df['throughput_smooth'].mean()
        else:
            avg_recovered = 0
        
        # Add text box with statistics
        stats_text = f'Avg Read Throughput:\n'
        stats_text += f'  Baseline: {avg_baseline:.1f} MB/s\n'
        stats_text += f'  During Recovery: {avg_degraded:.1f} MB/s\n'
        stats_text += f'  After Recovery: {avg_recovered:.1f} MB/s\n'
        
        if avg_baseline > 0:
            degradation = ((avg_baseline - avg_degraded) / avg_baseline) * 100
            stats_text += f'Degradation: {degradation:.1f}%'
        
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
                fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"\nâœ… Plot saved to: {output_file}")
    
    # Print summary
    print("\nðŸ“Š Experiment Summary:")
    print("=" * 60)
    for event_name, event_time in events.items():
        if event_name != 'start':
            elapsed = (event_time - events['start']).total_seconds()
            print(f"  {event_name:20s}: {elapsed:8.1f}s  ({event_time.strftime('%H:%M:%S')})")
    
    if 'failure' in events and 'recovery_complete' in events:
        recovery_duration = (events['recovery_complete'] - events['failure']).total_seconds()
        print(f"\n  Recovery Duration: {recovery_duration:.1f}s")
        print(f"\n  Average Read Throughput:")
        print(f"    Baseline:        {avg_baseline:.2f} MB/s")
        print(f"    During Recovery: {avg_degraded:.2f} MB/s")
        print(f"    After Recovery:  {avg_recovered:.2f} MB/s")
        if avg_baseline > 0:
            print(f"    Degradation:     {degradation:.1f}%")
    
    print("=" * 60)

if __name__ == '__main__':
    if len(sys.argv) > 1:
        result_dir = sys.argv[1]
    else:
        result_dir = os.path.dirname(os.path.abspath(__file__))
    
    print(f"Processing results from: {result_dir}")
    plot_experiment(result_dir)
